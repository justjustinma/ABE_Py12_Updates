{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABE tutorial 1\n",
    "## Setting up an ABE workshop\n",
    "\n",
    "In this first tutorial let's setup a workshop to build agents, environments, and RL algorthms!\n",
    "\n",
    "Steps:\n",
    "* Install cleanRL\n",
    "* Check that it works\n",
    "* Explore available algorithms and environments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean RL\n",
    "\n",
    "CleanRL is a python library that makes working with deep reinforcement learning easier. It's focus is on developing single file implimentations of reinforcement learning algorithms that can interact with a wide range of environments. You can read more of the documentation here: https://docs.cleanrl.dev/\n",
    "\n",
    "There are explicit installation steps to follow here (https://docs.cleanrl.dev/get-started/installation/). If you run into any trouble installing cleanrl go to those installation steps, otherwise you can try the following:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a virtual environment\n",
    "\n",
    "Creating a virtual environment is an easy way to make sure you can install all the right python libraries and versions for a specific project. First navigate to where you'd like to have your project and create a project folder.\n",
    "\n",
    "Open up a terminal (command line) and navigate to your folder. Once inside the folder we can create the virtual environment. To do so make sure you have a version of python installed on your computer that is between Python >=3.7.1,<3.11. \n",
    "\n",
    "On Ubuntu you can install a version of python via:\n",
    "```\n",
    "sudo add-apt-repository ppa:deadsnakes/ppa\n",
    "sudo apt install python3.10\n",
    "```\n",
    "\n",
    "Create the virtual environment by running the following:\n",
    "```\n",
    "python3.10 -m venv my_venv\n",
    "```\n",
    "\n",
    "Make sure to use the version of python you'd like to use with cleanrl (i.e., python3.10). The my_env is the name of your environment, feel free to change the name.\n",
    "\n",
    "You now have a virtual environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Install python libraries\n",
    "\n",
    "One of the main benifits of a virtual environment is that you can install all the libraries you need within it, without messing up any other python projects.\n",
    "\n",
    "The first step is to activate your virtual environment. This ensures that any libraries you install will be added to that python environment and not some other environment.\n",
    "\n",
    "Activate your virtual environment by typing the following into your terminal while inside your project folder (linux/mac):\n",
    "\n",
    "```\n",
    "source my_python_3_10_env/bin/activate\n",
    "```\n",
    "\n",
    "or with windows: \n",
    "```\n",
    "my_python_3_10_env\\Scripts\\activate\n",
    "```\n",
    "\n",
    "You should now see that on your terminal line you have a (my_env) in front of the line. Any libraries we now install will be inside this virtual environent!\n",
    "\n",
    "Let's install the libraries we'll need. Thankfully it's quite straightforward:\n",
    "\n",
    "```pip install cleanrl```\n",
    "\n",
    "It should take a little time to install cleanrl and all the libraries it depends on (e.g., torch, numpy, etc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check that it works!\n",
    "\n",
    "Here we'll do a quick check that it works. Run the following inside your terminal:\n",
    "\n",
    "```python -m cleanrl.ppo --env CartPole-v1```\n",
    "\n",
    "You should see output that shows how well a PPO agent is doing in the CartPole environment. You can stop the training by clicking ctrl-z.\n",
    "\n",
    "To see more about this environment you can check out: https://gymnasium.farama.org/environments/classic_control/cart_pole/ \n",
    "\n",
    "If this worked you can even try out another RL algorithm on the same environment:\n",
    "\n",
    "```python -m cleanrl.dqn --env CartPole-v1```\n",
    "\n",
    "Again you can click ctrl-z to stop the training.\n",
    "\n",
    "Next, let's look at how to work with cleanrl, e.g., modify/see the code, and how visualize the training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Working with cleanRL\n",
    "\n",
    "To work with cleanrl you can use any editor but were going to show instructions for how to use it with VSCode.\n",
    "\n",
    "Open VSCode and open your project folder. You should see your virtual environment folder inside, along with a runs folder. The runs folder contains all the runs you tested out in the steps before. Let's see those in a little more detail using tensorboard.\n",
    "\n",
    "In VSCode install the tensorboard extention. Click on extensions (left side bar), search for tensorboad, and click install.\n",
    "\n",
    "Now that we have it installed, and the folder we are in has a runs folder, we can run the tensoboard and it will display all the runs in the runs folder. To run tensorboard open the command palette (ctrl-shift-p or view-->command palette), type in tensorboard, and click on lauch tensorboard.\n",
    "\n",
    "It should show various graphs that we'll get to know better in subsequent tutorials about how the training went. Do you see any differences between the rl algorithms you ran in the previous step?\n",
    "\n",
    "Finally, let's see some of the code behind these algorithms. To see the ppo code, back in files (left sidebar), if you click on your virtual environment folder, you should see inside a cleanrl folder, inside that you should see the code for each of the algorithms. Click on PPO.py. This file contains all the steps to run and train a PPO agent. We'll get to know all the steps in this file! But for now its good enough to know where the code is and what it looks like!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Trying out some more examples\n",
    "\n",
    "If you'd like to play around a little more with agents and environments go check out the environments available:\n",
    "\n",
    " https://gymnasium.farama.org\n",
    "\n",
    " Scrol down and find the list of Environments, try running the PPO agent on some of them:\n",
    "\n",
    "\n",
    " ```python -m cleanrl.ppo --env Environment-name-here```\n",
    "\n",
    " e.g., \n",
    "```python -m cleanrl.ppo --env Acrobot-v1```\n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
