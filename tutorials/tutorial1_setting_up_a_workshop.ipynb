{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABE tutorial 1\n",
    "## Setting up an ABE workshop\n",
    "\n",
    "In this first tutorial let's setup a workshop to build agents, environments, and RL algorthms!\n",
    "\n",
    "Steps:\n",
    "* Install tianshou\n",
    "* Check that it works\n",
    "* Explore available algorithms and environments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tianshou\n",
    "\n",
    "Tianshou is a python library that makes working with deep reinforcement learning easier. It's focus is on developing implimentations of reinforcement learning algorithms that can interact with a wide range of environments. You can read more of the documentation here: https://tianshou.org\n",
    "\n",
    "There are some good tutorials on how to use the different modules of tianshou here: https://tianshou.org/en/stable/02_notebooks/L0_overview.html\n",
    "\n",
    "Below we'll cover most of what's covered in those tutorials here, but with a focus on what we are covering in the ABE book.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a virtual environment\n",
    "\n",
    "Creating a virtual environment is an easy way to make sure you can install all the right python libraries and versions for a specific project. First navigate to where you'd like to have your project and create a project folder.\n",
    "\n",
    "Open up a terminal (command line) and navigate to your folder. Once inside the folder we can create the virtual environment. \n",
    "\n",
    "First we have to make sure the right python distribution is installed. Tianshou requries python 3.11 or higher. \n",
    "\n",
    "You can check which python you are using with the command:\n",
    "\n",
    "```which python```\n",
    "\n",
    "On Ubuntu you can install a newer version of python via:\n",
    "\n",
    "```\n",
    "sudo add-apt-repository ppa:deadsnakes/ppa\n",
    "sudo apt install python3.11\n",
    "```\n",
    "\n",
    "Create the virtual environment by running the following:\n",
    "```\n",
    "python3.11 -m venv my_venv\n",
    "```\n",
    "Or if you already have a version 3.11 or greater then to create the virtual environment run the following:\n",
    "\n",
    "```\n",
    "python3 -m venv my_venv\n",
    "```\n",
    "\n",
    "The \"my_env\" is the name of your environment, feel free to change the name.\n",
    "\n",
    "You now have a virtual environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Install python libraries\n",
    "\n",
    "One of the main benifits of a virtual environment is that you can install all the libraries you need within it, without messing up any other python projects.\n",
    "\n",
    "The first step is to activate your virtual environment. This ensures that any libraries you install will be added to that python environment and not some other environment.\n",
    "\n",
    "Activate your virtual environment by typing the following into your terminal while inside your project folder (linux/mac):\n",
    "\n",
    "```\n",
    "source my_venv/bin/activate\n",
    "```\n",
    "\n",
    "or with windows: \n",
    "```\n",
    "my_venv\\Scripts\\activate\n",
    "```\n",
    "\n",
    "You should now see that on your terminal line you have a (my_env) in front of the line. Any libraries we now install will be inside this virtual environent!\n",
    "\n",
    "Let's install the libraries we'll need. Thankfully it's quite straightforward:\n",
    "\n",
    "```pip install tianshou```\n",
    "\n",
    "It should take a little time to install tianshou and all the libraries it depends on (e.g., torch, numpy, etc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check that it works!\n",
    "\n",
    "Here we'll do a quick check that the installation worked. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import tianshou\n",
    "print(tianshou.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train our first RL agent\n",
    "\n",
    "For this first time we'll focus just on the pieces that need to be in place for us to train a RL agent. Later as we move through the tutorials we'll learn more about each of the peices and even start to customize some of them!\n",
    "\n",
    "But for now let's use existing RL algorithms and some exsiting environments to just see how it all works together.\n",
    "\n",
    "To start off let's create a new python file, let's call it first_RL.py\n",
    "\n",
    "Import some libraries:\n",
    "\n",
    "* **gymnasium** will have some environments for us to use (https://gymnasium.farama.org/)\n",
    "* **torch** will let us build some neural networks (https://pytorch.org/)\n",
    "* **Tensorboard** will let us see how well our agent is doing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tianshou as ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then start a little logger so we can see what is going on. The code below will write out some summary statistics of our agent to the directory log/dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start a logger\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's choose an environment to train our agent in. We'll see how to work with environments and even how to make our own later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an envrironment: render mode = human means we'd like to see the environment.\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at this environment a litte. To do this let's reset the environment, then see what the agent can \"see\" and what actions the agent can take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start the environment at the \"start\"\n",
    "env.reset()\n",
    "\n",
    "#take a look at the environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Box' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#what can the agent see?\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Box' object is not callable"
     ]
    }
   ],
   "source": [
    "#what can the agent see?\n",
    "env.observation_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what can the agent do?\n",
    "env.action_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out some actions ourselves! Let's take 10 steps to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "[0] (<class 'list'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/Documents/GitHub/ABE/venv/lib/python3.11/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Documents/GitHub/ABE/venv/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/ABE/venv/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/ABE/venv/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:133\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mcontains(\n\u001b[1;32m    134\u001b[0m         action\n\u001b[1;32m    135\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(action)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) invalid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCall reset before using step method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m     x, x_dot, theta, theta_dot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
      "\u001b[0;31mAssertionError\u001b[0m: [0] (<class 'list'>) invalid"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    env.step([0])\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try taking more actions, can you stabilize the pole?\n",
    "\n",
    "Ok, let's train an agent to do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start building our agent. \n",
    "\n",
    "To start off let's build a neural network that take what the agent observes and converts that to actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import a network that we can use\n",
    "from tianshou.utils.net.common import Net\n",
    "\n",
    "#What the agent 'sees'\n",
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "\n",
    "#what actions the agent can take\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "\n",
    "#build a network that takes observations and converts it to actions\n",
    "net = Net(state_shape=state_shape, action_shape=action_shape, hidden_sizes=[128, 128, 128])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll need to get our agent to learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will shift the network to better predict actions/values\n",
    "optim = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ts.policy.DQNPolicy(\n",
    "    model=net,\n",
    "    optim=optim,\n",
    "    discount_factor=0.9,\n",
    "    action_space=env.action_space,\n",
    "    estimation_step=3,\n",
    "    target_update_freq=320\n",
    ")\n",
    "train_collector = ts.data.Collector(policy, env, ts.data.VectorReplayBuffer(20000, 1), exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, env, exploration_noise=True)  # because DQN uses epsilon-greedy method (chooses best action, with some noise epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 10001it [03:38, 45.71it/s, env_step=10000, gradient_step=1000, len=154, n/ep=0, n/st=10, rew=154.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 101.480000 ± 8.243155, best_reward: 101.480000 ± 8.243155 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 10001it [04:04, 40.90it/s, env_step=20000, gradient_step=2000, len=157, n/ep=0, n/st=10, rew=157.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 193.360000 ± 34.451856, best_reward: 193.360000 ± 34.451856 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 10001it [09:31, 17.50it/s, env_step=30000, gradient_step=3000, len=185, n/ep=0, n/st=10, rew=185.00]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 195.190000 ± 58.612745, best_reward: 195.190000 ± 58.612745 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 10001it [03:38, 45.86it/s, env_step=40000, gradient_step=4000, len=155, n/ep=0, n/st=10, rew=155.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 133.250000 ± 7.810730, best_reward: 195.190000 ± 58.612745 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 10001it [03:38, 45.86it/s, env_step=50000, gradient_step=5000, len=138, n/ep=0, n/st=10, rew=138.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 165.330000 ± 10.719193, best_reward: 195.190000 ± 58.612745 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 10001it [03:37, 45.89it/s, env_step=60000, gradient_step=6000, len=170, n/ep=0, n/st=10, rew=170.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 135.570000 ± 7.498340, best_reward: 195.190000 ± 58.612745 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 10001it [03:37, 45.88it/s, env_step=70000, gradient_step=7000, len=170, n/ep=0, n/st=10, rew=170.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 163.600000 ± 12.404032, best_reward: 195.190000 ± 58.612745 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 10001it [03:37, 45.90it/s, env_step=80000, gradient_step=8000, len=204, n/ep=0, n/st=10, rew=204.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 237.560000 ± 27.927162, best_reward: 237.560000 ± 27.927162 in #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 10001it [17:20,  9.62it/s, env_step=90000, gradient_step=9000, len=151, n/ep=0, n/st=10, rew=151.00]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 144.340000 ± 8.509078, best_reward: 237.560000 ± 27.927162 in #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 10001it [03:38, 45.84it/s, env_step=100000, gradient_step=10000, len=20, n/ep=0, n/st=10, rew=20.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 87.760000 ± 26.133549, best_reward: 237.560000 ± 27.927162 in #8\n",
      "Finished training in 6791.983935117722 seconds\n"
     ]
    }
   ],
   "source": [
    "result = ts.trainer.OffpolicyTrainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=10,\n",
    "    step_per_epoch=10000,\n",
    "    step_per_collect=10,\n",
    "    episode_per_test=100,\n",
    "    batch_size=64,\n",
    "    update_per_step=1 / 10,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(0.1),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(0.05),\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold,\n",
    "    logger=logger,\n",
    ").run()\n",
    "print(f\"Finished training in {result.timing.total_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the training write:\n",
    "\n",
    "launch tensorboard in the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/97/rrjknrxd3jb9wmkxbdkbvl800000gn/T/ipykernel_88705/2690805076.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy.load_state_dict(torch.load('dqn.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(policy.state_dict(), 'dqn.pth')\n",
    "policy.load_state_dict(torch.load('dqn.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch the trained agent in the environment\n",
    "\n",
    "To do so you might have to install pygame\n",
    "\n",
    "pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectStats(n_collected_episodes=1, n_collected_steps=98, collect_time=5.224884033203125, collect_speed=18.756397152018877, returns=array([98.]), returns_stat=SequenceSummaryStats(mean=98.0, std=0.0, max=98.0, min=98.0), lens=array([98]), lens_stat=SequenceSummaryStats(mean=98.0, std=0.0, max=98.0, min=98.0))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "policy.eval()\n",
    "policy.set_eps(0.05)\n",
    "collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
    "collector.collect(n_episode=1, render=1 / 35, reset_before_collect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.0050312 ,  0.5639927 , -0.06111142, -0.9501881 ]],\n",
       "       dtype=float32),\n",
       " array([1.]),\n",
       " array([False]),\n",
       " array([False]),\n",
       " array([{'env_id': 0}], dtype=object))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    env.step([1])\n",
    "    #env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        tuple\n",
      "\u001b[0;31mString form:\u001b[0m (array([-0.00426664,  0.00835143,  0.00373624, -0.01403215], dtype=float32), {})\n",
      "\u001b[0;31mLength:\u001b[0m      2\n",
      "\u001b[0;31mDocstring:\u001b[0m  \n",
      "Built-in immutable sequence.\n",
      "\n",
      "If no argument is given, the constructor returns an empty tuple.\n",
      "If iterable is specified the tuple is initialized from iterable's items.\n",
      "\n",
      "If the argument is a tuple, the return value is the same object."
     ]
    }
   ],
   "source": [
    "?obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        tuple\n",
      "\u001b[0;31mString form:\u001b[0m\n",
      "(array([[-0.00612657,  0.00564542, -0.01911404, -0.04796124]],\n",
      "      dtype=float32), array([{}], dtype=object))\n",
      "\u001b[0;31mLength:\u001b[0m      2\n",
      "\u001b[0;31mDocstring:\u001b[0m  \n",
      "Built-in immutable sequence.\n",
      "\n",
      "If no argument is given, the constructor returns an empty tuple.\n",
      "If iterable is specified the tuple is initialized from iterable's items.\n",
      "\n",
      "If the argument is a tuple, the return value is the same object."
     ]
    }
   ],
   "source": [
    "?obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'obs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#run a loop!\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#collector = ts.data.Collector(policy, env, exploration_noise=True)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#collector.collect(n_episode=1, render=1 / 35)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/ABE/venv/lib/python3.11/site-packages/tianshou/policy/modelfree/dqn.py:201\u001b[0m, in \u001b[0;36mDQNPolicy.forward\u001b[0;34m(self, batch, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute action over the given batch data.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03mIf you need to mask the action, please add a \"mask\" into batch.obs, for\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m    more detailed explanation.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, model)\n\u001b[0;32m--> 201\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# TODO: this is convoluted! See also other places where this is done.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m obs_next \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mobs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m obs\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'obs'"
     ]
    }
   ],
   "source": [
    "#create a new environment\n",
    "env = ts.env.SubprocVectorEnv([lambda: gym.make(\"CartPole-v1\", render_mode=\"human\") for _ in range(1)])\n",
    "\n",
    "\n",
    "#setup the policy\n",
    "policy.eval()\n",
    "policy.set_eps(0.05) #this gives it some randomness in choosing actions\n",
    "\n",
    "#get the initial observations\n",
    "obs = env.reset()\n",
    "\n",
    "#run a loop!\n",
    "for i in range(100):\n",
    "    policy.forward(obs)\n",
    "\n",
    "\n",
    "\n",
    "    #collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
    "    #collector.collect(n_episode=1, render=1 / 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectStats(n_collected_episodes=1, n_collected_steps=500, collect_time=26.80025601387024, collect_speed=18.65653819654668, returns=array([500.]), returns_stat=SequenceSummaryStats(mean=500.0, std=0.0, max=500.0, min=500.0), lens=array([500]), lens_stat=SequenceSummaryStats(mean=500.0, std=0.0, max=500.0, min=500.0))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You should see output that shows how well a PPO agent is doing in the CartPole environment. You can stop the training by clicking ctrl-z.\n",
    "\n",
    "To see more about this environment you can check out: https://gymnasium.farama.org/environments/classic_control/cart_pole/ \n",
    "\n",
    "If this worked you can even try out another RL algorithm on the same environment:\n",
    "\n",
    "```python -m cleanrl.dqn --env CartPole-v1```\n",
    "\n",
    "Again you can click ctrl-z to stop the training.\n",
    "\n",
    "Next, let's look at how to work with cleanrl, e.g., modify/see the code, and how visualize the training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Working with cleanRL\n",
    "\n",
    "To work with cleanrl you can use any editor but were going to show instructions for how to use it with VSCode.\n",
    "\n",
    "Open VSCode and open your project folder. You should see your virtual environment folder inside, along with a runs folder. The runs folder contains all the runs you tested out in the steps before. Let's see those in a little more detail using tensorboard.\n",
    "\n",
    "In VSCode install the tensorboard extention. Click on extensions (left side bar), search for tensorboad, and click install.\n",
    "\n",
    "Now that we have it installed, and the folder we are in has a runs folder, we can run the tensoboard and it will display all the runs in the runs folder. To run tensorboard open the command palette (ctrl-shift-p or view-->command palette), type in tensorboard, and click on lauch tensorboard.\n",
    "\n",
    "It should show various graphs that we'll get to know better in subsequent tutorials about how the training went. Do you see any differences between the rl algorithms you ran in the previous step?\n",
    "\n",
    "Finally, let's see some of the code behind these algorithms. To see the ppo code, back in files (left sidebar), if you click on your virtual environment folder, you should see inside a cleanrl folder, inside that you should see the code for each of the algorithms. Click on PPO.py. This file contains all the steps to run and train a PPO agent. We'll get to know all the steps in this file! But for now its good enough to know where the code is and what it looks like!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Trying out some more examples\n",
    "\n",
    "If you'd like to play around a little more with agents and environments go check out the environments available:\n",
    "\n",
    " https://gymnasium.farama.org\n",
    "\n",
    " Scrol down and find the list of Environments, try running the PPO agent on some of them:\n",
    "\n",
    "\n",
    " ```python -m cleanrl.ppo --env Environment-name-here```\n",
    "\n",
    " e.g., \n",
    "```python -m cleanrl.ppo --env Acrobot-v1```\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
