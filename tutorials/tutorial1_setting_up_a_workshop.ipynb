{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABE tutorial 1\n",
    "## Setting up an ABE workshop\n",
    "\n",
    "In this first tutorial let's setup a workshop to build agents, environments, and RL algorthms!\n",
    "\n",
    "Steps:\n",
    "* Install tianshou\n",
    "* Check that it works\n",
    "* Explore available algorithms and environments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tianshou\n",
    "\n",
    "Tianshou is a python library that makes working with deep reinforcement learning easier. It's focus is on developing implimentations of reinforcement learning algorithms that can interact with a wide range of environments. You can read more of the documentation here: https://tianshou.org\n",
    "\n",
    "There are some good tutorials on how to use the different modules of tianshou here: https://tianshou.org/en/stable/02_notebooks/L0_overview.html\n",
    "\n",
    "Below we'll cover most of what's covered in those tutorials here, but with a focus on what we are covering in the ABE book.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a virtual environment\n",
    "\n",
    "Creating a virtual environment is an easy way to make sure you can install all the right python libraries and versions for a specific project. First navigate to where you'd like to have your project and create a project folder.\n",
    "\n",
    "Open up a terminal (command line) and navigate to your folder. Once inside the folder we can create the virtual environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if you have python**\n",
    "\n",
    "First we have to make sure the right python distribution is installed. Tianshou requries python 3.11 or higher. \n",
    "\n",
    "You can check which python you are using with the command:\n",
    "\n",
    "Mac/Linux\n",
    "```which python```\n",
    "\n",
    "Windows\n",
    "```where python```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you don't have python, or an older version**\n",
    "\n",
    "\n",
    "On Ubuntu you can install a newer version of python via:\n",
    "\n",
    "```\n",
    "sudo add-apt-repository ppa:deadsnakes/ppa\n",
    "sudo apt install python3.11\n",
    "```\n",
    "\n",
    "On windows you can download and install python 3.11 or greater from: https://www.python.org/downloads/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a virtual environment**\n",
    "\n",
    "Create the virtual environment by running the following:\n",
    "```\n",
    "python3.11 -m venv my_venv\n",
    "```\n",
    "Or if you already using a version 3.11 or greater then to create the virtual environment run the following:\n",
    "\n",
    "```\n",
    "python -m venv my_venv\n",
    "```\n",
    "\n",
    "The \"my_env\" is the name of your environment, feel free to change the name.\n",
    "\n",
    "You now have a virtual environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Install python libraries\n",
    "\n",
    "One of the main benifits of a virtual environment is that you can install all the libraries you need within it, without messing up any other python projects.\n",
    "\n",
    "The first step is to activate your virtual environment. This ensures that any libraries you install will be added to that python environment and not some other environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activate your virtual environment**\n",
    "\n",
    "Activate your virtual environment by typing the following into your terminal while inside your project folder (linux/mac):\n",
    "\n",
    "```\n",
    "source my_venv/bin/activate\n",
    "```\n",
    "\n",
    "or with windows: \n",
    "```\n",
    "my_venv\\Scripts\\activate\n",
    "```\n",
    "\n",
    "You should now see that on your terminal line you have a (my_env) in front of the line. Any libraries we now install will be inside this virtual environent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install tianshou**\n",
    "\n",
    "Let's install the libraries we'll need. Thankfully it's quite straightforward:\n",
    "\n",
    "```pip install tianshou```\n",
    "\n",
    "It should take a little time to install tianshou and all the libraries it depends on (e.g., torch, numpy, etc)\n",
    "\n",
    "You might also need the library pygame, so let's install that too\n",
    "\n",
    "```pip install pygame```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check that it works!\n",
    "\n",
    "Here we'll do a quick check that the installation worked. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tianshou\n",
    "print(tianshou.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train our first RL agent\n",
    "\n",
    "For this first example we'll focus just on the pieces that need to be in place for us to train an RL agent. Later as we move through the tutorials we'll learn more about each of the peices and even start to customize some of them!\n",
    "\n",
    "But for now let's use existing RL algorithms and some exsiting environments to just see how it all works together.\n",
    "\n",
    "To start off let's create a new python file, let's call it first_RL.py\n",
    "\n",
    "Import some libraries:\n",
    "\n",
    "* **gymnasium** will have some environments for us to use (https://gymnasium.farama.org/)\n",
    "* **torch** will let us build some neural networks (https://pytorch.org/)\n",
    "* **Tensorboard** will let us see how well our agent is doing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tianshou as ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then start a \"logger\" so we can see what is going on. The code below will write out some summary statistics of our agent to the directory log/dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start a logger\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup an environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's choose an environment to train our agent in. We'll see how to work with environments and even how to make our own later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an envrironment: render mode = human means we'd like to see the environment.\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at this environment. To do this let's reset the environment, then see what the agent can \"see\" and what actions the agent can take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start the environment at the \"start\"\n",
    "env.reset()\n",
    "\n",
    "#take a look at the environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see a simple game pop up, with a brown pole being balanced by a black block. The agent moves the black block left or right in an attempt to keep the pole upright (i.e., less than 12 degrees from verticle).\n",
    "\n",
    "Let' take a look at what the agent sees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what can the agent see?\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it \"sees\" about 4 values:\n",
    "> Cart Position in the x-axis\n",
    "\n",
    "> Cart Velocity in the x-axis\n",
    "\n",
    "> Pole Angle from verticle\n",
    "\n",
    "> Pole Angular velocity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What actions can the agent take in this environment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what can the agent do?\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can take two discrete actions:\n",
    "\n",
    "> Push the cart to the left\n",
    "\n",
    "> Push the cart to the right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out some actions ourselves! Let's take 10 steps to the right.\n",
    "\n",
    "> A right action is coded as 0, and a left is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an output of the new observations after taking the step.\n",
    "\n",
    "Try running the code block above a few times, and change the actions. See if you can balance the pole!\n",
    "\n",
    "You can even try taking a few steps at a time using a loop below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try taking more actions, can you stabilize the pole? :-)\n",
    "\n",
    "Ok, let's train an agent to do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's close this instance of the environment for now... we'll open up one again below\n",
    "#but we'll leave the render_mode=\"human\" so that it runs faster (i.e., doesn't have to create an image for us to look at)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start building our agent. \n",
    "\n",
    "To start off let's build a neural network that take what the agent observes and converts that into actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import a network that we can use\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.space_info import SpaceInfo\n",
    "\n",
    "#get an instance of the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "#get all the info about it\n",
    "space_info = SpaceInfo.from_env(env)\n",
    "\n",
    "#What the agent 'sees'\n",
    "state_shape = space_info.observation_info.obs_shape\n",
    "\n",
    "#what actions the agent can take\n",
    "action_shape = space_info.action_info.action_shape\n",
    "\n",
    "#build a network that takes observations and converts it to actions\n",
    "net = Net(state_shape=state_shape, action_shape=action_shape, hidden_sizes=[128, 128, 128])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden_sizes argument above is for setting the number of nodes within each neural network layer that link the initial input (i.e., observations of the current state: 4) and the possible actions (i.e., 2 discrete actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll need to build an optimizer to allow our agent to learn! This optimizer will adjust the weights in the neural network to link observations to actions that lead to more rewards.\n",
    "\n",
    "In the case of the carte pole environment the rewards are the steps where the pole is held upright (i.e., less that 12 degrees from verticle).\n",
    "\n",
    "We'll use a pre-built optimizer called Adam, with a learning rate of 0.001. The learning rate determines how quickly the agent adapts it's weights during each step. This will be a hyperparameter that we will use more later in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will shift the network to better predict actions/values\n",
    "optim = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a network and an optimizer let's define a policy that will control how learning takes place.\n",
    "\n",
    "> The discount factor is how much the agent takes into acount future rewards vs. immediate rewards. A choice of 0.9 suggest that the agent should prioritize future rewards, while a choice of 0.1 suggests the agent should prioritize immediate rewards.\n",
    "\n",
    "> estimation_step is how many steps into the future the agent should look when calculating the value of different actions.\n",
    "\n",
    "> target_update_freq is how many steps should be taken before updating the network weights to match what the agent is learning.\n",
    "\n",
    "Some of these parameters are specific to the RL algorithm we are using here (i.e., estimation_step, and target_update_freq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ts.policy.DQNPolicy(\n",
    "    model=net,\n",
    "    optim=optim,\n",
    "    discount_factor=0.9,\n",
    "    action_space=env.action_space,\n",
    "    estimation_step=3,\n",
    "    target_update_freq=320\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's setup a collector to feed observations to the policy as the agent interacts with it's environment.\n",
    "\n",
    "> We'll add a test collector that will run tests periodically to see how well our agent is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector = ts.data.Collector(policy, env, ts.data.VectorReplayBuffer(20000, 1), exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, env, exploration_noise=True)  # because DQN uses epsilon-greedy method (chooses best action, with some noise epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have:\n",
    "\n",
    "1. An environment\n",
    "2. A Policy with a network model and an optimizer\n",
    "3. A collector to store the agent experiences\n",
    "\n",
    "We can now train our agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use something called an Off Policy Trainer for now. This trainer controls the learning of a main offline neural network model, and only periodically updates a second version of this neural network that is used by the agent to make descisions. This helps with the stability of the training/learning. However, we'll see in a few tutorials how we can have a fully online trainer where there is no distinction between an off line neural network model and the one being used by the agent to learn. \n",
    "\n",
    "Parameters:\n",
    "> max_epochs is the number of rounds of training to run before stopping the training.\n",
    "\n",
    "> steps_per_epoch is the number actions the agent will take per epoch\n",
    "\n",
    "> steps_per_collect is the number of actions to take before collecting experiences in the replay buffer (a list of stored experiences)\n",
    "\n",
    "> episode_per_test is the number of episodes to run during testing that occurs at the end of an epoch. This estimates how much our agent has learned.\n",
    "\n",
    "> batch_size is the amount of experiences to take from the replay buffer when trianing the neural network model.\n",
    "\n",
    "> train_fn is a function that is called at the start of each training epoch. Here it sets the eps (epsilon) paramter to 0.1. Telling the agent to try an exploritory action 10% of the time, rather than what the agent thinks is the best current action.\n",
    "\n",
    "> test_fn is the same as train_fn, just with the test environment.\n",
    "\n",
    "> stop_fn is a function that will stop the training if its conditions are met. Here is stops when the rewards reach a specific threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ts.trainer.OffpolicyTrainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=30,\n",
    "    step_per_epoch=10000,\n",
    "    step_per_collect=10,\n",
    "    episode_per_test=100,\n",
    "    batch_size=64,\n",
    "    update_per_step=1 / 10,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(0.1),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(0.05),\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold,\n",
    "    logger=logger,\n",
    ").run()\n",
    "print(f\"Finished training in {result.timing.total_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above can take a while to run! To see the training in progress you can launch tensorboard.\n",
    "\n",
    "If you are using VSCode you can open command pallette and write:\n",
    "\n",
    "```launch tensorboard```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the code has finished you can save the trained agent to be used later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), 'models/dqn.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then load the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.load_state_dict(torch.load('models/dqn.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works by watching the trained agent in a new environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a new environment, and make it start at the beginning\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.reset()\n",
    "\n",
    "#tell the policy that we are in evaluation mode\n",
    "policy.eval()\n",
    "\n",
    "#give some noise to the policy choices of actions\n",
    "policy.set_eps(0.05)\n",
    "\n",
    "#Create a collector\n",
    "collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
    "\n",
    "#Use the collector to run the agent and environment at 35 frames per second\n",
    "collector.collect(n_episode=1, render=1 / 35, reset_before_collect=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does your trained agent do? Was it better than you at keeping the pole upright?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close your environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to try**\n",
    "\n",
    "> Changing the environment to another classic control environment\n",
    "\n",
    ">> Go to https://gymnasium.farama.org/environments/classic_control/\n",
    "\n",
    ">> Choose another environment (it has to have discrete actions with the RL algorithm we are using here!): mountain car or acrobot. We'll learn in more depth other algorithms that can do both discrete and continuous actions.\n",
    "\n",
    ">> Use the code above as a guide and attempt to run an agent on one of these new environments below!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try out another environment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
