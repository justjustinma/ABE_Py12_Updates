{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABE tutorial 2\n",
    "## Looking at value based RL\n",
    "\n",
    "In this second tutorial let's dive deeper into the RL algorithm that we used in the first tutorial. This time let's open up the policy and see how it learns in more detail.\n",
    "\n",
    "Steps:\n",
    "* Create a custom policy\n",
    "* Check how it updates based on temporal difference learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup a simple RL example\n",
    "\n",
    "Let's build a simple RL example based on what we learnt last tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tianshou as ts\n",
    "from tianshou.utils.net.common import Net\n",
    "\n",
    "#start a logger\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn_custom'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the CartPole example an environment to train our agent in, and extract the observations (state_shape), and the action space (action_shape) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.utils.space_info import SpaceInfo\n",
    "\n",
    "# Create an envrironment: render mode = human means we'd like to see the environment.\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "#start the environment at the \"start\"\n",
    "env.reset()\n",
    "\n",
    "#get all the info about it\n",
    "space_info = SpaceInfo.from_env(env)\n",
    "\n",
    "#What the agent 'sees'\n",
    "state_shape = space_info.observation_info.obs_shape\n",
    "\n",
    "#what actions the agent can take\n",
    "action_shape = space_info.action_info.action_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(space_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start building our agent's brain. \n",
    "\n",
    "To start off let's build a neural network that take what the agent observes and converts that into actions. Then we'll add an optimizer to shift the weights in the network to better predict the value of states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#build a network that takes observations and converts it to actions\n",
    "net = Net(state_shape=state_shape, action_shape=action_shape, hidden_sizes=[64, 64])\n",
    "\n",
    "#this will shift the network to better predict actions/values\n",
    "optim = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a network and an optimizer let's define a policy that will control how learning takes place.\n",
    "\n",
    "Let's use a pre-built policy first (we'll open it up and take a look inside... but first let's get this all working!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ts.policy.DQNPolicy(\n",
    "    model=net,\n",
    "    optim=optim,\n",
    "    discount_factor=0.9,\n",
    "    action_space=env.action_space,\n",
    "    estimation_step=3,\n",
    "    target_update_freq=320\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's setup a collector to feed observations to the policy as the agent interacts with it's environment.\n",
    "\n",
    "> We'll add a test collector that will run tests periodically to see how well our agent is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector = ts.data.Collector(policy, env, ts.data.VectorReplayBuffer(20000, 1), exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, env, exploration_noise=True)  # because DQN uses epsilon-greedy method (chooses best action, with some noise epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have:\n",
    "\n",
    "1. An environment\n",
    "2. A Policy with a network model and an optimizer\n",
    "3. A collector to store the agent experiences\n",
    "\n",
    "We can now train our agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use an Off Policy Trainer for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ts.trainer.OffpolicyTrainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=10,\n",
    "    step_per_epoch=10000,\n",
    "    step_per_collect=30,\n",
    "    episode_per_test=100,\n",
    "    batch_size=64,\n",
    "    update_per_step=1 / 10,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(0.1),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(0.05),\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold,\n",
    "    logger=logger,\n",
    ").run()\n",
    "print(f\"Finished training in {result.timing.total_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above can take a while to run! To see the training in progress you can launch tensorboard.\n",
    "\n",
    "If you are using VSCode you can open command pallette and write:\n",
    "\n",
    "```launch tensorboard```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we'll build a custom policy to better understand how TD-learning is working and how our agents are learning!\n",
    "\n",
    "We'll also move from an offline training procedure, where the agent learns based on collected data after a training round, to an online training procedure, where the agent learns continuously on the fly in real time.  \n",
    "\n",
    "To do this we will build a model that will collect states, actions, and rewards as the agent is interacting with the environment. The agent will then use this information to learn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a custom policy we'll need to define how the agent will learn. To do this we'll create a new python class called SARSAPolicy. It will have three peices:\n",
    "\n",
    "* *__init__*: and initialization method\n",
    "* *forward*: a method that will take observations and predict actions, i.e., what action should the agent take? \n",
    "* *learn*: a method that will allow the agent to learn what the value of a state is, i.e., update the forward method\n",
    "\n",
    "```python\n",
    "# Custom SARSA policy class\n",
    "class SARSAPolicy(BasePolicy):\n",
    "    def __init__():\n",
    "        #initialize the policy, i.e., set some parameters values\n",
    "\n",
    "    def forward():\n",
    "        \n",
    "        #a model to predict actions\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn():\n",
    "\n",
    "        # Estimate the value of a state based on TD-learning\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the new policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the initialization of the policy first.\n",
    "\n",
    "When we initalize the training policy we'll provide information about:\n",
    "* the *model*, this will be used to choose actions given states\n",
    "* the *optim*, the optim (optimizer) will be used to update the model, i.e., it's how the agent will learn.\n",
    "* *gamma*, defines how the agent values future rewards vs. immediate rewards.\n",
    "* *epsilon*, defines the probability that the agent will select an action at random.\n",
    "\n",
    "```python\n",
    "        def __init__(self, model, optim, action_space, gamma=0.99, epsilon=0.1):\n",
    "                super().__init__(action_space=action_space)\n",
    "                self.model = model\n",
    "                self.optim = optim\n",
    "                self.gamma = gamma\n",
    "                self.epsilon = epsilon\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add the forward method that takes values of states and actions Q(s,a) and uses those to choose what actions to take.\n",
    "\n",
    "\n",
    "```python\n",
    "    def forward(self, batch, state=None, **kwargs):\n",
    "        \n",
    "        #enter the observations into the neural network model to estimate the value of actions\n",
    "        q_values, _ = self.model(batch.obs)\n",
    "\n",
    "        #choose the action with the highest value\n",
    "        act = q_values.argmax(dim=1).cpu().numpy()  # Greedy action\n",
    "\n",
    "        #every so often choose a random action\n",
    "        if np.random.rand() < self.epsilon:  # Epsilon-greedy exploration\n",
    "            act = np.random.randint(0, q_values.shape[1], size=act.shape)\n",
    "        \n",
    "        #return the chosen action\n",
    "        return Batch(act=act)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we initialized our policy with a model, an optimizer, and some paramters, as well as setup how the agent will choose actions using the model, we need to think about how the agent will learn. This is an important step. We'll use TD-learning here to update the model so that it can predict the value of actions. It will do this in a few steps:\n",
    "\n",
    "* Estimate the current state,action values: i.e., Q(s,a) or q_values.\n",
    "* Estimate the TD target: this is the expected value of the next state, action value.\n",
    "* Use the difference between the current q_value and td-target to update the neural network.\n",
    "* If we repeatedly do this the network should make better estimates of the q_values of state/action pairs.\n",
    "\n",
    "```python\n",
    "    def learn(self, batch, next_action, **kwargs):\n",
    "\n",
    "        #get the current estimate of state, action values\n",
    "        q_values, _ = self.model(batch.obs)\n",
    "\n",
    "        #get it into the right shape (this just ensures that the q_values are in a 1d tensor)\n",
    "        batch_act = torch.tensor(batch.act, dtype=torch.long) if not isinstance(batch.act, torch.Tensor) else batch.act\n",
    "        q_values = q_values.gather(1, batch_act.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Estiamte the TD target: this is what was actually seen\n",
    "        with torch.no_grad(): #this ensures that these calculations are not part of the optimization\n",
    "            \n",
    "            #predict what the value of state actions are going to be\n",
    "            next_q_values, _ = self.model(batch.obs_next) \n",
    "            next_q_values = next_q_values.gather(1, next_action.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            #TD is then the reward observed plus the value of the q_values in the next state\n",
    "            td_target = batch.rew + self.gamma * (1 - batch.done) * next_q_values\n",
    "\n",
    "        #now that we know the TD target, how close did our estimate of q_values get to the target?\n",
    "        #with TD learning the value of a state is tied to the value of the next state.\n",
    "\n",
    "        # calculate the difference between the current state action pariing (q_value) and the td target (based on the next state,action) \n",
    "        loss = nn.functional.mse_loss(q_values, td_target) #using MSE to measure the difference\n",
    "        \n",
    "        #make sure the optimizer is set back to zero (reset)\n",
    "        self.optim.zero_grad()\n",
    "        \n",
    "        #based on the loss use backpropogation to adjust the neural network weights to make better predictions\n",
    "        loss.backward()\n",
    "\n",
    "        #limit the amount that weights can change\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        \n",
    "        #Adjust the weights in the neural network\n",
    "        self.optim.step()\n",
    "\n",
    "        return {\"loss\": loss.item()}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the policy we have to define a neural network (i.e., the model) that the policy can use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nerual Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how to create an agent brain. To do this we'll create a new class called QNet. It will have two methods:\n",
    "\n",
    "* *___init___*: this will initialize the network model\n",
    "* *forward*: this will build a model that will take as input the state and output the action chosen.\n",
    "\n",
    "```python\n",
    "class QNet(nn.Module):\n",
    "    \n",
    "    def __init__():\n",
    "\n",
    "        #initialize the neural netork model\n",
    "\n",
    "    def forward(obs):\n",
    "        \n",
    "        #take observations (i.e., state) and make predictions about the value of actions (i.e., q_values)\n",
    "\n",
    "        return q_values, state\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's take a look at the initialization first. When we initalize the agent's brain we'll provide information about:\n",
    "* the state_shape, this will be the size of the observation space, i.e., how many varaibles act as inputs for the agent?\n",
    "* the action_shape, the number of actions the agent can take.\n",
    "* hidden_size, defines the number of nodes in each hidden layer of the neural network.\n",
    "\n",
    "```python    \n",
    "    #define the information needed to inialize the policy\n",
    "    def __init__(self, state_shape, action_shape, hidden_size=128):\n",
    "        super().__init__()\n",
    "```\n",
    "\n",
    "The super().__init__() initializes the network based on pytorches nn.Module. Once this is done, let's add a neural network that the agent can use to predict the best action to take in the current context. Note: here we are only using the current state to make predictions about actions, we'l see later on how we can add in a trajectory of states from the recent past to help our agent make better decision in environments that are dynamic and stochastic.\n",
    "\n",
    "```python\n",
    "        #build a neural network model\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape), hidden_size), #also know as a dense layer\n",
    "            nn.ReLU(),                          #Restricted linear unit activation function\n",
    "            nn.LayerNorm(hidden_size),          #normalizes the edge weights (stops the weights from getting too big)\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, np.prod(action_shape))\n",
    "        )\n",
    "```\n",
    "\n",
    "We'll cover how to build your own neural networks a little further along in the tutorials, but for now it's enough to know that we are building a neural network with 3 layers, using a RELU function to connect them, and doing some layerNormalization to avoid large changes in edge weights. The input will be the observed variables and the output will be the action taken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's look at how to build the *forward* method to predict actions:\n",
    "\n",
    "```python\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        \n",
    "        #convert observations to tensor (this is like an array but can be faster!)\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        \n",
    "        #use the neural network to predict value of actions (i.e., q_values)\n",
    "        q_values = self.net(obs)\n",
    "\n",
    "        #return the estiamted value of actions and the state\n",
    "        return q_values, state\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gone through the code peice by peice, let's take a look at that full code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.data import Batch, ReplayBuffer, Collector\n",
    "from tianshou.policy import BasePolicy\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, state_shape, action_shape, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape), hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, np.prod(action_shape))\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        q_values = self.net(obs)\n",
    "        return q_values, state\n",
    "\n",
    "# Custom SARSA policy class\n",
    "class SARSAPolicy(BasePolicy):\n",
    "    def __init__(self, model, optim, action_space, gamma=0.99, epsilon=0.1):\n",
    "        super().__init__(action_space=action_space)\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, batch, state=None, **kwargs):\n",
    "        q_values, _ = self.model(batch.obs)\n",
    "        act = q_values.argmax(dim=1).cpu().numpy()  # Greedy action\n",
    "        if np.random.rand() < self.epsilon:  # Epsilon-greedy exploration\n",
    "            act = np.random.randint(0, q_values.shape[1], size=act.shape)\n",
    "        return Batch(act=act)\n",
    "\n",
    "    def learn(self, batch, next_action, **kwargs):\n",
    "\n",
    "        #use the model to get the current q_values\n",
    "        q_values, _ = self.model(batch.obs)\n",
    "        batch_act = torch.tensor(batch.act, dtype=torch.long) if not isinstance(batch.act, torch.Tensor) else batch.act\n",
    "        q_values = q_values.gather(1, batch_act.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Estimate TD target\n",
    "        with torch.no_grad():\n",
    "            next_q_values, _ = self.model(batch.obs_next)\n",
    "            next_q_values = next_q_values.gather(1, next_action.unsqueeze(1)).squeeze(1)\n",
    "            td_target = batch.rew + self.gamma * (1 - batch.done) * next_q_values\n",
    "\n",
    "        # Use the difference between q_value and TD-target to update the model\n",
    "        loss = nn.functional.mse_loss(q_values, td_target)\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        self.optim.step()\n",
    "\n",
    "        return {\"loss\": loss.item()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment\n",
    "env =  gym.make(\"CartPole-v1\") \n",
    "test_env = gym.make(\"CartPole-v1\") \n",
    "\n",
    "#get the environment information\n",
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.n if hasattr(env.action_space, 'n') else env.action_space.shape\n",
    "action_space = env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build our network model, optimizer, and place both into our custom policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Q-network and SARSA policy\n",
    "net = QNet(state_shape, action_shape)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "policy = SARSAPolicy(model=net, optim=optimizer, action_space=action_space, gamma=0.99, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the pieces in place, let's train our agent. However, let's take a online approach, and get our agent to learn on the fly! To do this we will create a custom training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom training loop\n",
    "max_epoch = 10\n",
    "step_per_epoch = 5000\n",
    "keep_n_steps = 30\n",
    "buffer = ReplayBuffer(size=keep_n_steps)\n",
    "\n",
    "# Set up collectors to store data\n",
    "train_collector = Collector(policy, env, buffer)\n",
    "test_collector = Collector(policy, test_env)\n",
    "\n",
    "#start a logger\n",
    "logger_sarsa = ts.utils.TensorboardLogger(SummaryWriter('log/sarsa_custom'))\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    \n",
    "    #for each epoch reset the collector\n",
    "    train_collector.reset()\n",
    "    \n",
    "    for step in range(step_per_epoch):\n",
    "        \n",
    "        # Collect one transition and store it in the buffer\n",
    "        train_collector.collect(n_step=keep_n_steps)\n",
    "\n",
    "        # Sample the most recent observations from the buffer\n",
    "        batch = train_collector.buffer[-keep_n_steps:]\n",
    "        \n",
    "        # Manually convert each field to a torch tensor (for some reason to_torch isn't working here...)\n",
    "        batch.obs = torch.tensor(batch.obs, dtype=torch.float32)\n",
    "        batch.act = torch.tensor(batch.act, dtype=torch.long)\n",
    "        batch.rew = torch.tensor(batch.rew, dtype=torch.float32)\n",
    "        batch.done = torch.tensor(batch.done, dtype=torch.float32)\n",
    "        batch.obs_next = torch.tensor(batch.obs_next, dtype=torch.float32)\n",
    "\n",
    "        # Choose next action based on epsilon-greedy policy using `obs_next`\n",
    "        next_action = policy.forward(Batch(obs=batch.obs_next)).act\n",
    "\n",
    "        # Perform SARSA learning\n",
    "        policy.learn(batch, torch.tensor(next_action))\n",
    "\n",
    "    # Testing and evaluation\n",
    "    result = test_collector.collect(n_episode=10, reset_before_collect=True)\n",
    "    print(f'Epoch #{epoch + 1}: reward = {result.returns.mean()}, loss = {policy.learn(batch, torch.tensor(next_action))[\"loss\"]}')\n",
    "    \n",
    "    # Log the average reward for the epoch\n",
    "    logger.writer.add_scalar(\"Reward/test_avg\", result.returns.mean(), epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it learn? Do you see rewards increasing? \n",
    "\n",
    "If so let's save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"models/sarsa_cartpole_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the model, and watch what it learnt.\n",
    "\n",
    "Load in the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new network with the same architecture\n",
    "loaded_net = QNet(state_shape, action_shape)\n",
    "loaded_net.load_state_dict(torch.load(\"models/sarsa_cartpole_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an environment and build a policy based on our saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment for evaluation with rendering enabled\n",
    "eval_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Set the loaded network as the model for a new SARSA policy\n",
    "loaded_policy = SARSAPolicy(model=loaded_net, optim=optimizer, action_space=action_space, gamma=0.99, epsilon=0.0)  # Set epsilon=0 for pure exploitation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our agent in the environment. Note: you can change the number of episodes to watch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the number of episodes you want to watch\n",
    "num_episodes = 1\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = eval_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    print(f\"Starting episode {episode + 1}\")\n",
    "\n",
    "    while not done:\n",
    "        # Create a batch for the current observation\n",
    "        obs_batch = Batch(obs=[obs])\n",
    "        \n",
    "        # Get action based on loaded model's Q-values (no exploration)\n",
    "        action = loaded_policy.forward(obs_batch).act[0]\n",
    "        \n",
    "        # Step the environment with the selected action\n",
    "        obs, reward, done, truncated, _ = eval_env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Check if the episode has ended\n",
    "        if done or truncated:\n",
    "            print(f\"Episode {episode + 1} ended with total reward: {total_reward}\")\n",
    "            break  # Break out of the loop to start the next episode\n",
    "\n",
    "\n",
    "# Close the environment after finishing all episodes\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to try**\n",
    "\n",
    "Try changing the environment or changing the hyperparameters:\n",
    "\n",
    "* **Epsilon** (when to explore!): too high the agent cannot take advantage of the environment as they are acting randomly, too low the agent might get stuck learning the first useful thing rather than finding the best actions.\n",
    "\n",
    "* **Learning rate** (how fast to learn from new data): too high and the agent might learn sprious correlations between actions and outcomes, too low and it might take the agent for ever to figure what actions lead to good rewards.\n",
    "\n",
    "* **Discount factor** or **gamma** (how much does the agent value future vs. near rewards): too high and the agent might miss near rewards, too low and the agent might be too focused on the short term and miss longer term outcomes.\n",
    "\n",
    "Try altering some of these hyperparameters and see how that changes the ability of your agent to learn! Which hyperparameters work best?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
