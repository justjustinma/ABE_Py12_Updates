{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABE tutorial 3\n",
    "## Looking at actor critic based RL\n",
    "\n",
    "In this third tutorial let's dive deeper into the RL algorithm that we used in the first and second tutorials. This time we'll learn how we can have two models: the critic model that will be dedicated to estimating the value of states, and the actor model that will help predict which actions will lead to more rewards in the long term (i.e., a policy).\n",
    "\n",
    "Steps:\n",
    "* Create an Actor-critic neural network\n",
    "* Create a custom Policy and training procedure\n",
    "* Test out the Actor-critic RL algorithm in an environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off one of the main differences between the SARSA algorithm in the last tutorial and the Actor-Critic model we'll be learning here is that we need two models:\n",
    "\n",
    "* Actor: Finds the probabilities of actions in states that leads to the highest return in rewards.\n",
    "* Critic: Finds the expected returns of being in a state.\n",
    "\n",
    "Just like with the SARSA code let's create a new class:\n",
    "\n",
    "```python\n",
    "class ActorCriticNet(nn.Module):\n",
    "    \n",
    "    #initialize the network\n",
    "    def __init__(self, state_shape, action_shape, hidden_size=128):\n",
    "                \n",
    "        #actor model\n",
    "        self.actor = nn.Sequential(\n",
    "        )\n",
    "\n",
    "        #critic model\n",
    "        self.critic = nn.Sequential(\n",
    "        )\n",
    "        \n",
    "        \n",
    "    #get predictions from the models\n",
    "    def forward():\n",
    "                \n",
    "        return action, state_value\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the parts let's fill in the code. Below we'll define some network layers for the critic and actor models.\n",
    "\n",
    "\n",
    "```python\n",
    "class ActorCriticNet(nn.Module):\n",
    "    \n",
    "    #initialize the network\n",
    "    def __init__(self, state_shape, action_shape, hidden_size=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        #build the actor model\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape), hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, np.prod(action_shape))\n",
    "        )\n",
    "\n",
    "        #build the actor model\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape), hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "```\n",
    "\n",
    "Next let's build the method that will get predictions from the actor-critic network models\n",
    "\n",
    "```python\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        \n",
    "        #make sure observations are in tensor format\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "                \n",
    "        # Actor network output: logits for actions, i.e., how likely are the actions\n",
    "        action_logits = self.actor(obs)\n",
    "        \n",
    "        # Critic network output: state value\n",
    "        state_value = self.critic(obs).squeeze(-1)\n",
    "        \n",
    "        return action_logits, state_value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we'll build a custom policy to better understand how TD-learning is working with our actor-critic algorithm! We'll see that the policy stays very close to the SARSA policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a custom policy we'll need to define how the agent will learn. To do this we'll create a new python class called A2CPolicy. It will have three peices:\n",
    "\n",
    "* *__init__*: and initialization method\n",
    "* *forward*: a method that will take observations and predict actions, i.e., what action should the agent take? \n",
    "* *learn*: a method that will allow the agent to learn what the value of a state is, i.e., update the forward method\n",
    "\n",
    "```python\n",
    "# Custom SARSA policy class\n",
    "class A2CPolicy(BasePolicy):\n",
    "    def __init__():\n",
    "        #initialize the policy, i.e., set some parameters values\n",
    "\n",
    "    def forward():\n",
    "        \n",
    "        #a model to predict actions\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn():\n",
    "\n",
    "        # Estimate the value of a state based on TD-learning\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the new policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the initialization of the policy first.\n",
    "\n",
    "When we initalize the training policy we'll provide information about:\n",
    "* the *model*, this will be used to choose actions given states\n",
    "* the *optim*, the optim (optimizer) will be used to update the model, i.e., it's how the agent will learn.\n",
    "* *gamma*, defines how the agent values future rewards vs. immediate rewards.\n",
    "* *Note:*, epsilon is no longer needed as our actor outputs a distribution of probabilities (logit scale) that defines the probability that the agent will select an action. So in this way our A2C algorithm already has exploration built in and does not need to take random actions.\n",
    "\n",
    "```python\n",
    "        def __init__(self, model, optim, action_space, gamma=0.99):\n",
    "                super().__init__(action_space=action_space)\n",
    "                self.model = model\n",
    "                self.optim = optim\n",
    "                self.gamma = gamma\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add the forward method that takes the probabilities of action (on the logit scale) and uses those to choose what actions to take.\n",
    "\n",
    "* **Note**, The categorical distribution below is a good choice when the actions are discrete actions. We'll see how to modify this so that we can also work with continuous actions spaces.\n",
    "\n",
    "\n",
    "```python\n",
    "    def forward(self, batch, state=None, **kwargs):\n",
    "        logits, _ = self.model(batch.obs)\n",
    "        \n",
    "        # Sample action based on policy (softmax)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        return Batch(act=action.cpu().numpy(), dist=dist)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we initialized our policy with a model, an optimizer, and some paramters, as well as setup how the agent will choose actions using the model, we need to think about how the agent will learn. This is an important step. We'll use TD-learning here to update the model so that it can predict the value of actions. It will do this in a few steps:\n",
    "\n",
    "* Estimate the current state value and the action probabilities.\n",
    "* Estimate the TD target: this is the expected value of the next state, action value.\n",
    "* Calculate the advantage: this measures how much better our predicted state values are when compared to what was seen before.\n",
    "* Use the difference between the current action probabilities and the advantage of each action to make the actor better. The ideal here is if the action probabilities exactly match the expected value returns of each action.\n",
    "* Use the difference between the estimated state value and the TD-target to make the critic better.\n",
    "* If we repeatedly do this the actor and critic network should make better estimates action probabilities, and state values.\n",
    "\n",
    "```python\n",
    "    def learn(self, batch, **kwargs):\n",
    "        \n",
    "        # Forward pass to get actor (logits) and critic (value)\n",
    "        logits, state_values = self.model(batch.obs)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        \n",
    "        # Compute the log probabilities of the taken actions\n",
    "        log_probs = dist.log_prob(batch.act)\n",
    "        \n",
    "        # Compute the critic's next state values (for TD target)\n",
    "        with torch.no_grad():\n",
    "            _, next_state_values = self.model(batch.obs_next)\n",
    "            td_target = batch.rew + self.gamma * (1 - batch.done) * next_state_values\n",
    "            \n",
    "            # Calculate the normalized advantage\n",
    "            advantage = td_target - state_values  # Advantage calculation\n",
    "            advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8) #normalization\n",
    "\n",
    "        # Calculate entropy for the policy distribution\n",
    "        #entropy = dist.entropy().mean()\n",
    "        \n",
    "        # Calculate policy (actor) loss (include entropy regularization)\n",
    "        policy_loss = -(log_probs * advantage.detach()).mean() #- 0.01 * entropy  # Adjust weight as needed\n",
    "        \n",
    "        # Calculate value (critic) loss\n",
    "        value_loss = nn.functional.mse_loss(state_values, td_target)\n",
    "        \n",
    "        # Combine the losses\n",
    "        loss = policy_loss + value_loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0) #gradient clipping: make sure the updates to the network are small.\n",
    "        self.optim.step()\n",
    "\n",
    "        return {\"loss\": loss.item(), \"policy_loss\": policy_loss.item(), \"value_loss\": value_loss.item()}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the policy and an actor/critic model, let's take a look at the full code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gone through the code peice by peice, let's take a look at that full code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.data import Batch, ReplayBuffer, Collector\n",
    "from tianshou.policy import BasePolicy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tianshou as ts\n",
    "\n",
    "\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, state_shape, action_shape, hidden_size=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape), hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, np.prod(action_shape))\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape), hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        \n",
    "        # Actor network output: logits for actions\n",
    "        action_logits = self.actor(obs)\n",
    "        \n",
    "        # Critic network output: state value\n",
    "        state_value = self.critic(obs).squeeze(-1)\n",
    "        \n",
    "        return action_logits, state_value\n",
    "\n",
    "\n",
    "class A2CPolicy(BasePolicy):\n",
    "    def __init__(self, model, optim, action_space, gamma=0.99):\n",
    "        super().__init__(action_space=action_space)\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, batch, state=None, **kwargs):\n",
    "        logits, _ = self.model(batch.obs)\n",
    "        \n",
    "        # Sample action based on policy (softmax)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        return Batch(act=action.cpu().numpy(), dist=dist)\n",
    "\n",
    "    def learn(self, batch, **kwargs):\n",
    "        \n",
    "        # Forward pass to get actor (logits) and critic (value)\n",
    "        logits, state_values = self.model(batch.obs)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        \n",
    "        # Compute the log probabilities of the taken actions\n",
    "        log_probs = dist.log_prob(batch.act)\n",
    "        \n",
    "        # Compute the critic's next state values (for TD target)\n",
    "        with torch.no_grad():\n",
    "            _, next_state_values = self.model(batch.obs_next)\n",
    "            td_target = batch.rew + self.gamma * (1 - batch.done) * next_state_values\n",
    "            \n",
    "            #calculate the normalized advantage\n",
    "            advantage = td_target - state_values  # Advantage calculation\n",
    "            advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "\n",
    "        # Calculate entropy for the policy distribution\n",
    "        #entropy = dist.entropy().mean()\n",
    "        \n",
    "        # Calculate policy (actor) loss (include entropy regularization)\n",
    "        policy_loss = -(log_probs * advantage.detach()).mean() #- 0.01 * entropy  # Adjust weight as needed\n",
    "        \n",
    "        # Calculate value (critic) loss\n",
    "        value_loss = nn.functional.mse_loss(state_values, td_target)\n",
    "        \n",
    "        # Combine the losses\n",
    "        loss = policy_loss + value_loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optim.step()\n",
    "\n",
    "        return {\"loss\": loss.item(), \"policy_loss\": policy_loss.item(), \"value_loss\": value_loss.item()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out the A2C algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test out our new A2C algorithm, let's create an environment and train an agent in it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single environment instance to access the space information\n",
    "single_env = gym.make(\"CartPole-v1\")\n",
    "state_shape = single_env.observation_space.shape \n",
    "action_shape = single_env.action_space.n\n",
    "action_space = single_env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's built the Actor-critic network and the optimizer to allow the network to learn. Then lets put that all in the new A2CPolicy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the actor-critic network and A2C policy\n",
    "net = ActorCriticNet(state_shape, action_shape)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "policy = A2CPolicy(model=net, optim=optimizer, action_space=action_space, gamma=0.99)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run a training loop so that the agent can interact with the environment, and we can store obserations and use the observations to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Custom training loop\n",
    "max_epoch = 10\n",
    "step_per_epoch = 5000\n",
    "keep_n_steps = 30\n",
    "buffer = ReplayBuffer(size=keep_n_steps)\n",
    "\n",
    "# Set up collectors\n",
    "train_collector = Collector(policy, single_env, buffer)\n",
    "test_collector = Collector(policy, single_env)\n",
    "\n",
    "#start a logger\n",
    "logger_a2c = ts.utils.TensorboardLogger(SummaryWriter('log/a2c_custom'))\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    train_collector.reset()\n",
    "    for step in range(step_per_epoch):\n",
    "        # Collect one transition and store it in the buffer\n",
    "        #train_collector.collect(n_step=1)\n",
    "        train_collector.collect(n_step=keep_n_steps)\n",
    "\n",
    "        # Sample the most recent observations from the buffer\n",
    "        #batch, _ = train_collector.buffer.sample(batch_size=30)\n",
    "        batch = train_collector.buffer[-keep_n_steps:]\n",
    "\n",
    "        # Manually convert each field to a torch tensor\n",
    "        batch.obs = torch.tensor(batch.obs, dtype=torch.float32)\n",
    "        batch.act = torch.tensor(batch.act, dtype=torch.long)\n",
    "        batch.rew = torch.tensor(batch.rew, dtype=torch.float32)\n",
    "        batch.done = torch.tensor(batch.done, dtype=torch.float32)\n",
    "        batch.obs_next = torch.tensor(batch.obs_next, dtype=torch.float32)\n",
    "\n",
    "        # Perform A2C learning\n",
    "        policy.learn(batch)\n",
    "\n",
    "    # Testing and evaluation\n",
    "    result = test_collector.collect(n_episode=10, reset_before_collect=True)\n",
    "    print(f'Epoch #{epoch + 1}: reward = {result.returns.mean()}, loss = {policy.learn(batch)[\"loss\"]}')\n",
    "\n",
    "    # Log the average reward for the epoch\n",
    "    logger_a2c.writer.add_scalar(\"Reward/test_avg\", result.returns.mean(), epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it learn? Do you see rewards increasing? \n",
    "\n",
    "If so let's save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"models/A2C_cartpole_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the model, and watch what it learnt.\n",
    "\n",
    "Load in the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new network with the same architecture\n",
    "loaded_net = ActorCriticNet(state_shape, action_shape)\n",
    "loaded_net.load_state_dict(torch.load(\"models/A2C_cartpole_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an environment and build a policy based on our saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment for evaluation with rendering enabled\n",
    "eval_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Set the loaded network as the model for a new SARSA policy\n",
    "loaded_policy = A2CPolicy(model=loaded_net, optim=optimizer, action_space=action_space, gamma=0.99)  # Set epsilon=0 for pure exploitation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our agent in the environment. Note: you can change the number of episodes to watch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the number of episodes you want to watch\n",
    "num_episodes = 1\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = eval_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    print(f\"Starting episode {episode + 1}\")\n",
    "\n",
    "    while not done:\n",
    "        # Create a batch for the current observation\n",
    "        obs_batch = Batch(obs=[obs])\n",
    "        \n",
    "        # Get action based on loaded model's Q-values (no exploration)\n",
    "        action = loaded_policy.forward(obs_batch).act[0]\n",
    "        \n",
    "        # Step the environment with the selected action\n",
    "        obs, reward, done, truncated, _ = eval_env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Check if the episode has ended\n",
    "        if done or truncated:\n",
    "            print(f\"Episode {episode + 1} ended with total reward: {total_reward}\")\n",
    "            break  # Break out of the loop to start the next episode\n",
    "\n",
    "\n",
    "# Close the environment after finishing all episodes\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to try**\n",
    "\n",
    "Try changing the environment or changing the hyperparameters:\n",
    "\n",
    "* **Learning rate** (how fast to learn from new data): too high and the agent might learn sprious correlations between actions and outcomes, too low and it might take the agent for ever to figure what actions lead to good rewards.\n",
    "\n",
    "* **Discount factor** or **gamma** (how much does the agent value future vs. near rewards): too high and the agent might miss near rewards, too low and the agent might be too focused on the short term and miss longer term outcomes.\n",
    "\n",
    "Try altering some of these hyperparameters and see how that changes the ability of your agent to learn! Which hyperparameters work best?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
