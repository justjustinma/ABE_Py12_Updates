{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABE tutorial 4\n",
    "## Using functional approximation\n",
    "\n",
    "In this fourth tutorial let's dive deeper into how we use neural network models with RL. Now that we understand the concept of functional approximation, we can see how these neural network models are operating. We'll spend some time on some of the options when it comes to building neural networks for functional approximation, and go over some computational hurdles. \n",
    "\n",
    "Steps:\n",
    "* Go over the building of a neural network\n",
    "* Normalization\n",
    "* Converting A2C to continous action spaces\n",
    "* Test out the continuous A2C algorithm in new environments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use pytorch as our python package to build neural networks. We've seen these before in our first 4 tutorials, but here let's dive into the details a little more.\n",
    "\n",
    "The first thing to note is that we are using a sequential approach to building our neural networks. In this approach we just need to specify a network by providing an ordered list of layers. Let's take a look at how to do this below, by building a simple three layer network:\n",
    "\n",
    "* **Input layer**: this is the layer where the data comes into the model. Let's assume there are 4 input variables.\n",
    "\n",
    "* **First hidden layer**: this a layer of nodes that is connected to the input layer and will transform the input data, and pass these transformed values to the output layer. Let's assume this hidden layer has 32 nodes.\n",
    "\n",
    "* **Output layer**: this output layer will take the transformed values and output values that can be used to inform what actions can be taken. Let's assume there are two actions that can be taken.\n",
    "\n",
    "You should see below these 3 layers, and you should see how each layers shape coresponds to the data: e.g., 4 input values gets passed to the 32 nodes in the hidden layer, and how those 32 nodes pass those transformed values to the 2 actions in the output layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import hiddenlayer as hl\n",
    "\n",
    "#build a simple two three layer model\n",
    "my_net = nn.Sequential(\n",
    "            nn.Linear(4, 32),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "print(my_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a little more about the network we can install torchsummary. Just make sure you are still in your virtual environment and run the following.\n",
    "\n",
    "```\n",
    "pip install torchsummary\n",
    "```\n",
    "\n",
    "Then you should be able to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Specify the input shape as a tuple\n",
    "input_shape = (1, 4)\n",
    "\n",
    "# Print the summary\n",
    "summary(my_net, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the  summary above that the model has 1282 parameters! These are all the weights and bias values that are associated with each edge.\n",
    "\n",
    "In the book, we saw that these weights and biases on their own are really just linear equations applied to some inputs... and that to capture non-linear relationships we had to introduce activation functions. Let's do that now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a simple two three layer model\n",
    "my_net_2 = nn.Sequential(\n",
    "            nn.Linear(4, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "summary(my_net_2, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By placing the RELU activation layers after each layer we are filtering out any nodes that are outputing negative values. This cutoff is what let's a neural network model non-linear relationships.\n",
    "\n",
    "You'll notice that the model has the same number of weights and biases paramters. This is because the activation function is really just a filter and requires no new parameters. \n",
    "\n",
    "You'll notice too that there is no activation function applied after the output layer. This is because we want the output layer to output a continuous value and we want to keep negative values as an option. We'll see that for the output layer we have to think more about what kinds of outputs we want (continuous numeric, restricted to be between 0-1, ...etc) and that will determine how we build this last layer. Internally, however, with the hidden layers we will generally use RELU activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some useful layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many kinds of layers we can build into our networks, we'll learn a few as we build our agents. Generally, these layers solve some problem for us, or allow our agents to experience the world in a different way. \n",
    "\n",
    "In this tutorial we'll learn about the Normalization layer. This layer solves a problem for us. As our agents are continuously learning and adjusting weights/biases in their neural networks, the size of those weights can get quite large, making subsequent changes to those weights/biases harder to adapt when learning. To make sure the weights/biases don't get to large, and allow our agents to be more flexible in learning, we will normalize the values of the weights/biases. This will still mean that larger weights and smaller weights will still be relatively the same their magnitudes will be reduced. This solves, or helps to solve, the computational issues of having very large weights/biases.\n",
    "\n",
    "Let's see how to add that into our network. With the sequential approach we just have to stack the new layers in like lego blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a simple two three layer model\n",
    "my_net_3 = nn.Sequential(\n",
    "            nn.Linear(4, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "summary(my_net_3, input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have more parameters in this normalized model. These new parameters are used to normalize the weights/biases of each layer.\n",
    "\n",
    "Again we don't add the layerNorm to the output layer as the magnitudes of output are meaningful and we want to keep these magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen how we can build neural networks using a lego like approach and using different kinds of layers. Let's see now how we can update the weights/biases of these layers so that the network can learn. To do this, let's:\n",
    "\n",
    "* Simulate some data to use as input\n",
    "* Measure how far the network predictions are from the \"right\" answer\n",
    "* Adjust the weights and biases to make better predcitions\n",
    "* Do this many times, until the network is makeing good predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate some data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will look at how we can handle actions that are continuous. Up until now we've been relying on actions being e.g., ;eft, right, rather thane.g., motion adjusted by -0.23. The difference here is that there are not discrete actions to choose from, rather some amount of action in continuous space. This will be very usful as our agent bodies and their abilities to interact with their environment become more open ended. That is, we want to allow our agent to find many different ways to interact with the environment, and we don't want to constrain the agent to a few discrete actions. This has some costs, as it is much easier to learn how to use discrete actions, and is a way for us to help our agents to learn faster if there is some limit to their actions. We'll see that similar to discrete cases, we can add in some information/constraints in continuous action spaces to help our agent learn.\n",
    "\n",
    "First let's see where we need to alter our A2C agent to allow for continuous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's start with the actor network:\n",
    "\n",
    "```python\n",
    "\n",
    "self.actor = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape), hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, np.prod(action_shape))\n",
    "        )\n",
    "\n",
    "```\n",
    "\n",
    "This model can remain the same, as it will output a continuous value for each action space. We'll use this value as the mean of a guassian distribution. \n",
    "\n",
    "We'll then have to add an additional parameter for the standard deviation of the guassian distribution.\n",
    "\n",
    "```python\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(np.prod(action_shape)), requires_grad=True)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next in the forward pass, where our model is used to make predictions about which actions to take, we'll have to modify how those actions are chosen.\n",
    "\n",
    "```python\n",
    "        # Actor network outputs: mean and std deviation for Gaussian distribution\n",
    "        action_mean = self.actor(base_features)\n",
    "        action_log_std = self.actor_log_std.clamp(-20, 2)  # Clipping for numerical stability\n",
    "        action_std = action_log_std.exp()  # Convert log std to std\n",
    "```\n",
    "\n",
    "Then when we return the action choice we keep both the mean and the std of the actions:\n",
    "\n",
    "```python\n",
    "return action_mean, action_std, state_value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy adjustments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When taking actions we'll have to adjust how these actions are chosen:\n",
    "\n",
    "```python\n",
    "def forward(self, batch, state=None, **kwargs):\n",
    "\n",
    "        #run the model and get the action means and the uncertainty (std)\n",
    "        action_mean, action_std, _ = self.model(batch.obs)\n",
    "        \n",
    "        # Create Gaussian distribution for continuous actions\n",
    "        dist = torch.distributions.Normal(action_mean, action_std)\n",
    "\n",
    "        #sample actions from the Gaussian distribution\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # Clip actions to be within the environment’s action space: i.e., make sure the actions make sense / are possible\n",
    "        action = torch.clamp(action, self.action_space.low[0], self.action_space.high[0])\n",
    "        \n",
    "        return Batch(act=action.cpu().numpy(), dist=dist)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the learning section of the policy we need to calculate the log probability of each action in a slightly different way now that we have continuous actions.\n",
    "\n",
    "```python\n",
    "    def learn(self, batch, **kwargs):\n",
    "        \n",
    "        # Forward pass to get mean, std, and value\n",
    "        action_mean, action_std, state_values = self.model(batch.obs)\n",
    "        dist = torch.distributions.Normal(action_mean, action_std)\n",
    "        \n",
    "        # Compute log probabilities of the taken actions\n",
    "        log_probs = dist.log_prob(batch.act).sum(dim=-1)\n",
    "\n",
    "        #... the rest of the policy code stays the same\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out our new A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.data import Batch, ReplayBuffer, Collector\n",
    "from tianshou.policy import BasePolicy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tianshou as ts\n",
    "\n",
    "\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, state_shape, action_shape, hidden_size=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape), hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, np.prod(action_shape))\n",
    "        )\n",
    "\n",
    "        # Separate layer for log std to allow independent learning\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(np.prod(action_shape)), requires_grad=True)\n",
    "\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape), hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "        # Actor network outputs: mean and std deviation for Gaussian distribution\n",
    "        action_mean = self.actor(obs)\n",
    "        action_log_std = self.actor_log_std.clamp(-20, 2)  # Clipping for numerical stability\n",
    "        action_std = action_log_std.exp()  # Convert log std to std\n",
    "        \n",
    "        # Critic network output: state value\n",
    "        state_value = self.critic(obs).squeeze(-1)\n",
    "        \n",
    "        return action_mean, action_std, state_value\n",
    "\n",
    "\n",
    "class A2CPolicy(BasePolicy):\n",
    "    def __init__(self, model, optim, action_space, gamma=0.99):\n",
    "        super().__init__(action_space=action_space)\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, batch, state=None, **kwargs):\n",
    "        action_mean, action_std, _ = self.model(batch.obs)\n",
    "        \n",
    "        # Create Gaussian distribution for continuous actions\n",
    "        dist = torch.distributions.Normal(action_mean, action_std)\n",
    "\n",
    "        # Sample an action from the guassian distribution\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # Clip actions to be within the environment’s action space: i.e., make sure the action is possible\n",
    "        action = torch.clamp(action, self.action_space.low[0], self.action_space.high[0])\n",
    "        \n",
    "        return Batch(act=action.cpu().numpy(), dist=dist)\n",
    "\n",
    "    def learn(self, batch, **kwargs):\n",
    "        \n",
    "        # Forward pass to get mean, std, and value\n",
    "        action_mean, action_std, state_values = self.model(batch.obs)\n",
    "        dist = torch.distributions.Normal(action_mean, action_std)\n",
    "        \n",
    "        # Compute log probabilities of the taken actions\n",
    "        log_probs = dist.log_prob(batch.act).sum(dim=-1)\n",
    "\n",
    "        # Compute the critic's next state values (for TD target)\n",
    "        with torch.no_grad():\n",
    "            _, _, next_state_values = self.model(batch.obs_next)\n",
    "            td_target = batch.rew + self.gamma * (1 - batch.done) * next_state_values\n",
    "            \n",
    "            # Calculate the normalized advantage\n",
    "            advantage = td_target - state_values  # Advantage calculation\n",
    "            advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "\n",
    "\n",
    "        # Calculate entropy for the policy distribution\n",
    "        #entropy = dist.entropy().mean()\n",
    "        \n",
    "        # Calculate policy (actor) loss (include entropy regularization)\n",
    "        policy_loss = -(log_probs * advantage.detach()).mean() #- 0.01 * entropy  # Adjust weight as needed\n",
    "        \n",
    "        # Calculate value (critic) loss\n",
    "        value_loss = nn.functional.mse_loss(state_values, td_target)\n",
    "        \n",
    "        # Combine the losses\n",
    "        loss = policy_loss + value_loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optim.step()\n",
    "\n",
    "        return {\"loss\": loss.item(), \"policy_loss\": policy_loss.item(), \"value_loss\": value_loss.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single environment instance to access the space information\n",
    "single_env = gym.make(\"MountainCarContinuous-v0\")\n",
    "state_shape = single_env.observation_space.shape \n",
    "action_shape = single_env.action_space.shape #change n to shape\n",
    "action_space = single_env.action_space\n",
    "\n",
    "\n",
    "# Setting up the actor-critic network and A2C policy\n",
    "net = ActorCriticNet(state_shape, action_shape)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "policy = A2CPolicy(model=net, optim=optimizer, action_space=action_space, gamma=0.99)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom training loop\n",
    "max_epoch = 10\n",
    "step_per_epoch = 10000\n",
    "keep_n_steps = 30\n",
    "buffer = ReplayBuffer(size=keep_n_steps)\n",
    "\n",
    "# Set up collectors\n",
    "train_collector = Collector(policy, single_env, buffer)\n",
    "test_collector = Collector(policy, single_env)\n",
    "\n",
    "#start a logger\n",
    "logger_a2c = ts.utils.TensorboardLogger(SummaryWriter('log/a2c_cont_custom'))\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    train_collector.reset()\n",
    "    for step in range(step_per_epoch):\n",
    "        # Collect one transition and store it in the buffer\n",
    "        #train_collector.collect(n_step=1)\n",
    "        train_collector.collect(n_step=keep_n_steps)\n",
    "\n",
    "        # Sample the most recent observations from the buffer\n",
    "        #batch, _ = train_collector.buffer.sample(batch_size=30)\n",
    "        batch = train_collector.buffer[-keep_n_steps:]\n",
    "\n",
    "        # Manually convert each field to a torch tensor\n",
    "        batch.obs = torch.tensor(batch.obs, dtype=torch.float32)\n",
    "        batch.act = torch.tensor(batch.act, dtype=torch.long)\n",
    "        batch.rew = torch.tensor(batch.rew, dtype=torch.float32)\n",
    "        batch.done = torch.tensor(batch.done, dtype=torch.float32)\n",
    "        batch.obs_next = torch.tensor(batch.obs_next, dtype=torch.float32)\n",
    "\n",
    "        # Perform A2C learning\n",
    "        policy.learn(batch)\n",
    "\n",
    "    # Testing and evaluation\n",
    "    result = test_collector.collect(n_episode=10, reset_before_collect=True)\n",
    "    print(f'Epoch #{epoch + 1}: reward = {result.returns.mean()}, loss = {policy.learn(batch)[\"loss\"]}')\n",
    "\n",
    "    # Log the average reward for the epoch\n",
    "    logger_a2c.writer.add_scalar(\"Reward/test_avg\", result.returns.mean(), epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it learn? Do you see rewards increasing? \n",
    "\n",
    "If so let's save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"models/A2C_mountain_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the model, and watch what it learnt.\n",
    "\n",
    "Load in the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new network with the same architecture\n",
    "loaded_net = ActorCriticNet(state_shape, action_shape)\n",
    "loaded_net.load_state_dict(torch.load(\"models/A2C_mountain_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an environment and build a policy based on our saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment for evaluation with rendering enabled\n",
    "eval_env = gym.make(\"MountainCarContinuous-v0\", render_mode=\"human\")\n",
    "\n",
    "# Set the loaded network as the model for a new SARSA policy\n",
    "loaded_policy = A2CPolicy(model=loaded_net, optim=optimizer, action_space=action_space, gamma=0.99)  # Set epsilon=0 for pure exploitation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our agent in the environment. Note: you can change the number of episodes to watch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the number of episodes you want to watch\n",
    "num_episodes = 10\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = eval_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    print(f\"Starting episode {episode + 1}\")\n",
    "\n",
    "    while not done:\n",
    "        # Create a batch for the current observation\n",
    "        obs_batch = Batch(obs=[obs])\n",
    "        \n",
    "        # Get action based on loaded model's Q-values (no exploration)\n",
    "        action = loaded_policy.forward(obs_batch).act[0]\n",
    "        \n",
    "        # Step the environment with the selected action\n",
    "        obs, reward, done, truncated, _ = eval_env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Check if the episode has ended\n",
    "        if done or truncated:\n",
    "            print(f\"Episode {episode + 1} ended with total reward: {total_reward}\")\n",
    "            break  # Break out of the loop to start the next episode\n",
    "\n",
    "\n",
    "# Close the environment after finishing all episodes\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to try**\n",
    "\n",
    "Try changing the environment or changing the hyperparameters:\n",
    "\n",
    "* **Learning rate** (how fast to learn from new data): too high and the agent might learn sprious correlations between actions and outcomes, too low and it might take the agent for ever to figure what actions lead to good rewards.\n",
    "\n",
    "* **Discount factor** or **gamma** (how much does the agent value future vs. near rewards): too high and the agent might miss near rewards, too low and the agent might be too focused on the short term and miss longer term outcomes.\n",
    "\n",
    "Try altering some of these hyperparameters and see how that changes the ability of your agent to learn! Which hyperparameters work best?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that actions can be continuous this opens up the possibility of more complex agent bodies. Let's try mujoco!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single environment instance to access the space information\n",
    "single_env = gym.make(\"HalfCheetah-v4\")\n",
    "state_shape = single_env.observation_space.shape \n",
    "action_shape = single_env.action_space.shape #change n to shape\n",
    "action_space = single_env.action_space\n",
    "\n",
    "\n",
    "# Setting up the actor-critic network and A2C policy\n",
    "net = ActorCriticNet(state_shape, action_shape)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "policy = A2CPolicy(model=net, optim=optimizer, action_space=action_space, gamma=0.99)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
